<a name="SbHR1"></a>
# 简介
在 ZooKeeper 中，提供了三种 Leader 选举的算法，分别是 LeaderElection、UDP版本的FastLeaderElection 和 TCP版本的 FastLeaderElection，可以通过在配置文件 zoo.cfg 中使用electionAlg 属性来指定，分别使用数字0-3来表示。0代表 LeaderElection，这是一种纯UDP实现的 Leader选举算法，1代表UDP版本的 FastLeaderElection，并且是非授权模式，2也代表 UDP 版本的 FastLeaderElection，但使用授权模式，3代表 TCP版本的 FastLeaderElection。值得一提的是，从3.4.0版本开始，ZooKeeper 废弃了 0、1和2这三种 Leader 选举算法，只保留了 TCP版本的FastLeaderElection 选举算法。

<a name="B3NU7"></a>
## 术语约定
<a name="kKL20"></a>
### SID: 服务器ID
SID是一个数字，用来唯一标识一台 ZooKeeper 集群中的机器，每台机器不能重复，和myid的值一致。
<a name="Nq7tS"></a>
### ZXID:事务ID
ZXID 是一个事务 ID，用来唯一标识一次服务器状态的变更。在某一个时刻，集群中每台机器的ZXID值不一定全都一致，这和 ZooKeeper 服务器对于客户端“更新请求”的处理逻辑有关。
<a name="OsvC6"></a>
### Vote:投票
Leader 选举，顾名思义必须通过投票来实现。当集群中的机器发现自己无法检测到Leader 机器的时候，就会开始尝试进行投票。
<a name="U8Gh4"></a>
### Quorum:过半机器数
Leader 选举算法中最重要的一个术语，这个术语理解为是一个量词指的是 ZooKeeper 集群中过半的机器数，如果集群中总的机器数是 n的话，那么可以通过下面这个公式来计算 quorum 的值:<br />quorum = n/2+1

<a name="LdQ0n"></a>
### 变更投票
集群中的每台机器发出自己的投票后，也会接收到来自集群中其他机器的投票。每台机器都会根据一定的规则，来处理收到的其他机器的投票，并以此来决定是否需要变更自己的投票。这个规则也成为了整个 Leader 选举算法的核心所在。为了便于描述，我们首先定义一些术语。

- vote_sid:接收到的投票中所推举 Leader 服务器的 SID
- vote_zxid:接收到的投票中所推举 Leader 服务器的ZXID
- self sid:当前服务器自己的 SID
- self_zxid:当前服务器自己的ZXID

每次对于收到的投票的处理，都是一个对(vote_sid,vote_zxid)和(self_sid,self_zxid)对比的过程

- 规则1:如果 vote_zxid 大于 self_zxid，就认可当前收到的投票，并再次将该投票发送出去
> 假设，有三个服务器：a,b,c，如果 vote_zxid(c,2) > self_zxid(a,1) ，那么 self_zxid(a,2)
> self_zxid(a,2) -> vote_zxid(c,2)


- 规则2:如果 vote_zxid 小于self_zxid，那么就坚持自己的投票，不做任何变更
- 规则3:如果 vote_zxid 等于 self_zxid，那么就对比两者的SID。如果 vote_sid 大于self_sid，那么就认可当前接收到的投票，并再次将该投票发送出去
> 假设，有三个服务器：a,b,c，如果 vote_zxid(a,2) == self_zxid(c,2) ， a > c , self_zxid(a,2)
> self_zxid(a,2) -> vote_zxid(a,2)

- 规则4:如果 vote_zxid 等于 self_zxid，并且 vote_sid 小于 self_sid，那么同样坚持自己的投票，不做变更
> 假设，有三个服务器：a,b,c，如果 vote_zxid(a,2) == self_zxid(c,2) ， a < c , self_zxid(c,2)


<a name="UBOHl"></a>
### 服务器状态
在 org.apache.zookeeper.server.quorum.QuorumPeer.ServerState类中列举了4种服务器状态，分别是：LOOKING、FOLLOWING、LEADING和OBSERVING。

- LOOKING:寻找 Leader 状态。当服务器处于该状态时，它会认为当前集群中没有Leader，因此需要进入 Leader选举流程
- FOLLOWING:跟随者状态，表明当前服务器角色是 Follower
- LEADING:领导者状态，表明当前服务器角色是 Leader
- OBSERVING:观察者状态，表明当前服务器角色是 Observer

<a name="Wg3XM"></a>
# Leader 选举
<a name="lkK0C"></a>
## 核心 API
<a name="EGHKK"></a>
### 选票 - org.apache.zookeeper.server.quorum.Vote
```java
public class Vote {
    ...
    private final int version;

    private final long id;

    private final long zxid;

    private final long electionEpoch;

    private final long peerEpoch;
    ...
}
```

<a name="X14Ws"></a>
### 仲裁成员 - org.apache.zookeeper.server.quorum.QuorumPeer
此类管理仲裁协议。此服务器可能处于以下三种状态：

- 领导者选举-每个服务器将选举一个领导者（最初提议自己作为领导者）
- 追随者-服务器将与领导者同步并复制任何事务
- 领导者-服务器将处理请求并将其转发给追随者。大多数追随者必须先记录请求，然后才能接受请求

这个类将设置一个数据报套接字，该套接字将始终以当前前导的视图进行响应。回应将采取以下形式：
```java
   int xid;
  
   long myid;
  
   long leader_id;
  
   long leader_zxid;
```
<a name="zHpzm"></a>
#### 服务器状态 - org.apache.zookeeper.server.quorum.QuorumPeer.ServerState
```java
    public enum ServerState {
        LOOKING,
        FOLLOWING,
        LEADING,
        OBSERVING
    }
```
<a name="Gty9r"></a>
#### ZAB 状态 - org.apache.zookeeper.server.quorum.QuorumPeer.ZabState
```java
    public enum ZabState {
        ELECTION,
        DISCOVERY,
        SYNCHRONIZATION,
        BROADCAST
    }
```
<a name="ZlY9k"></a>
#### 同步状态 - org.apache.zookeeper.server.quorum.QuorumPeer.SyncMode
```java
    public enum SyncMode {
        NONE,
        DIFF,
        SNAP,
        TRUNC
    }
```
<a name="MnT3w"></a>
#### Leader 类型 - org.apache.zookeeper.server.quorum.QuorumPeer.LearnerType
```java
    public enum LearnerType {
        PARTICIPANT,
        OBSERVER
    }
```

<a name="FcJGL"></a>
### 仲裁校验器 - org.apache.zookeeper.server.quorum.flexible.QuorumVerifier
<a name="Hi0zl"></a>
#### 多数仲裁校验器 - org.apache.zookeeper.server.quorum.flexible.QuorumMaj
<a name="DsZTd"></a>
#### 神谕仲裁校验器 - org.apache.zookeeper.server.quorum.flexible.QuorumOracleMaj
<a name="aaJQc"></a>
#### 层次仲裁校验器 - org.apache.zookeeper.server.quorum.flexible.QuorumHierarchical

<a name="xx5vL"></a>
### Zookeeper 服务器 - org.apache.zookeeper.server.ZooKeeperServer
<a name="pphk8"></a>
#### 核心方法
<a name="dAv0t"></a>
##### 安装 RequestProcessor 链 - setupRequestProcessors
```java
    protected void setupRequestProcessors() {
        RequestProcessor finalProcessor = new FinalRequestProcessor(this);
        RequestProcessor syncProcessor = new SyncRequestProcessor(this, finalProcessor);
        ((SyncRequestProcessor) syncProcessor).start();
        firstProcessor = new PrepRequestProcessor(this, syncProcessor);
        ((PrepRequestProcessor) firstProcessor).start();
    }
```
<a name="Qfth8"></a>
### 抽象 Zookeeper 仲裁服务器 - org.apache.zookeeper.server.quorum.QuorumZooKeeperServer
参与仲裁的所有ZooKeeperServer的抽象基类
<a name="lfQGr"></a>
#### Leader 实现类 - org.apache.zookeeper.server.quorum.LeaderZooKeeperServer 
就像标准的ZooKeeperServer一样。我们只是替换请求处理器：PrepRequestProcessor->ProposalRequestProcessor->CommitProcessor->Leader.ToBeAppliedRequestProcessor->FinalRequestProcessor
<a name="q75jK"></a>
##### 安装 RequestProcessor 链 - setupRequestProcessors
```java
   @Override
    protected void setupRequestProcessors() {
        RequestProcessor finalProcessor = new FinalRequestProcessor(this);
        RequestProcessor toBeAppliedProcessor = new Leader.ToBeAppliedRequestProcessor(finalProcessor, getLeader());
        commitProcessor = new CommitProcessor(toBeAppliedProcessor, Long.toString(getServerId()), false, getZooKeeperServerListener());
        commitProcessor.start();
        ProposalRequestProcessor proposalProcessor = new ProposalRequestProcessor(this, commitProcessor);
        proposalProcessor.initialize();
        prepRequestProcessor = new PrepRequestProcessor(this, proposalProcessor);
        prepRequestProcessor.start();
        firstProcessor = new LeaderRequestProcessor(this, prepRequestProcessor);

        setupContainerManager();
    }
```
<a name="cbHE1"></a>
#### Follower 实现类 - org.apache.zookeeper.server.quorum.FollowerZooKeeperServer
就像标准的ZooKeeperServer一样。我们只是替换了请求处理器：FollowerRequestProcessor->CommitProcessor->FinalRequestProcessor还派生了一个SyncRequestProcessor来记录领导者的提案。
<a name="HWJgD"></a>
##### 安装 RequestProcessor 链 - setupRequestProcessors
```java
    @Override
    protected void setupRequestProcessors() {
        RequestProcessor finalProcessor = new FinalRequestProcessor(this);
        commitProcessor = new CommitProcessor(finalProcessor, Long.toString(getServerId()), true, getZooKeeperServerListener());
        commitProcessor.start();
        firstProcessor = new FollowerRequestProcessor(this, commitProcessor);
        ((FollowerRequestProcessor) firstProcessor).start();
        syncProcessor = new SyncRequestProcessor(this, new SendAckRequestProcessor(getFollower()));
        syncProcessor.start();
    }
```
<a name="TVN1E"></a>
#### Observer 实现类 - org.apache.zookeeper.server.quorum.ObserverZooKeeperServer
用于Observer节点类型的ZooKeeperServer。没有什么不同，但我们预计未来会专门处理请求处理器。
<a name="mFyZe"></a>
##### 安装 RequestProcessor 链 - setupRequestProcessors
```java
    @Override
    protected void setupRequestProcessors() {
        // We might consider changing the processor behaviour of
        // Observers to, for example, remove the disk sync requirements.
        // Currently, they behave almost exactly the same as followers.
        RequestProcessor finalProcessor = new FinalRequestProcessor(this);
        commitProcessor = new CommitProcessor(finalProcessor, Long.toString(getServerId()), true, getZooKeeperServerListener());
        commitProcessor.start();
        firstProcessor = new ObserverRequestProcessor(this, commitProcessor);
        ((ObserverRequestProcessor) firstProcessor).start();

        /*
         * Observer should write to disk, so that the it won't request
         * too old txn from the leader which may lead to getting an entire
         * snapshot.
         *
         * However, this may degrade performance as it has to write to disk
         * and do periodic snapshot which may double the memory requirements
         */
        if (syncRequestProcessorEnabled) {
            syncProcessor = new SyncRequestProcessor(this, null);
            syncProcessor.start();
        }
    }
```
<a name="kkC6v"></a>
#### Learner 实现类 - org.apache.zookeeper.server.quorum.LearnerZooKeeperServer
适用于学习者的所有ZooKeeperServer的父类
<a name="QEmYC"></a>
### 快速 Leader 选举 - FastLeaderElection
使用TCP实现领导人选举。它使用类QuorumCnxManager的对象来管理连接。否则，该算法与其他UDP实现一样是基于推送的。有一些参数可以调整以改变其行为。首先，finalizeWait确定在决定领导者之前需要等待的时间。这是领导人选举算法的一部分。

<a name="OeDPn"></a>
#### 通知 - org.apache.zookeeper.server.quorum.FastLeaderElection.Notification
通知是一种消息，让其他对 Peer 知道给定的 Peer 已经更改了投票，要么是因为它加入了领导人选举，要么是由于它得知另一个具有更高zxid或相同zxid和更高服务器id的 Peer
```java
   public static class Notification {
        /*
         * Format version, introduced in 3.4.6
         */

        public static final int CURRENTVERSION = 0x2;
        int version;

        /*
         * Proposed leader
         */ long leader;

        /*
         * zxid of the proposed leader
         */ long zxid;

        /*
         * Epoch
         */ long electionEpoch;

        /*
         * current state of sender
         */ QuorumPeer.ServerState state;

        /*
         * Address of sender
         */ long sid;

        QuorumVerifier qv;
        /*
         * epoch of the proposed leader
         */ long peerEpoch;

    }
```
<a name="OiIck"></a>
#### 发送消息 - org.apache.zookeeper.server.quorum.FastLeaderElection.ToSend
对等方想要发送给其他对等方的消息。这些消息既可以是通知，也可以是接收到通知的确认。
```java
        /*
         * Proposed leader in the case of notification
         */ long leader;

        /*
         * id contains the tag for acks, and zxid for notifications
         */ long zxid;

        /*
         * Epoch
         */ long electionEpoch;

        /*
         * Current state;
         */ QuorumPeer.ServerState state;

        /*
         * Address of recipient
         */ long sid;

        /*
         * Used to send a QuorumVerifier (configuration info)
         */ byte[] configData = dummyData;

        /*
         * Leader epoch
         */ long peerEpoch;
```
<a name="E5h2F"></a>
#### 消息处理器 - org.apache.zookeeper.server.quorum.FastLeaderElection.Messenger
消息处理程序的多线程实现。Messenger实现了两个子类：WorkReceiver和WorkSender。从名称中可以明显看出每一个的功能。每一个都会产生一个新的线程。
<a name="Yenwt"></a>
#### 核心方法
<a name="ih0pK"></a>
##### 查找 Leader - lookForLeader
<a name="drspS"></a>
###### 1. 注册 JMX 服务器
```java
    try {
        self.jmxLeaderElectionBean = new LeaderElectionBean();
        MBeanRegistry.getInstance().register(self.jmxLeaderElectionBean, self.jmxLocalPeerBean);
    } catch (Exception e) {
        LOG.warn("Failed to register with JMX", e);
        self.jmxLeaderElectionBean = null;
    }
```
<a name="WXtgV"></a>
###### 2.  初始化局部变量
```java
    /*
     * The votes from the current leader election are stored in recvset. In other words, a vote v is in recvset
     * if v.electionEpoch == logicalclock. The current participant uses recvset to deduce on whether a majority
     * of participants has voted for it.
     */
    Map<Long, Vote> recvset = new HashMap<Long, Vote>();
    
    /*
     * The votes from previous leader elections, as well as the votes from the current leader election are
     * stored in outofelection. Note that notifications in a LOOKING state are not stored in outofelection.
     * Only FOLLOWING or LEADING notifications are stored in outofelection. The current participant could use
     * outofelection to learn which participant is the leader if it arrives late (i.e., higher logicalclock than
     * the electionEpoch of the received notifications) in a leader election.
     */
    Map<Long, Vote> outofelection = new HashMap<Long, Vote>();
    
    int notTimeout = minNotificationInterval;
```

- recvset - 当前领导人选举的选票存储在recvset中。换句话说，如果v.electionEpoch== logicalclock，则投票v在recvset中。当前参与者使用recvset来推断是否有大多数参与者投了赞成票。
- outofelection - 以前的领导人选举的选票，以及现在的领导人选举中的选票都存储在选举外。请注意，处于LOOKING状态的通知不会存储在outofelection中。只有FOLLOWING或LEADING通知存储在outofelection中。如果当前参与者在领导人选举中延迟（即，logicalclock 高于收到通知的选举Epoch），则可以使用outofelection来了解哪个参与者是领导人。
- notTimeout : 通知超时时间（单位：毫秒），默认 minNotificationInterval = finalizeWait = 200
<a name="rvaHp"></a>
###### 3. 更新逻辑时钟以及 Proposal 状态，并发送给其他节点
```java
    synchronized (this) {
        logicalclock.incrementAndGet();
        updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());
    }

    LOG.info(
        "New election. My id = {}, proposed zxid=0x{}",
        self.getMyId(),
        Long.toHexString(proposedZxid));
    sendNotifications();
```
```java
	synchronized void updateProposal(long leader, long zxid, long epoch) {
        LOG.debug(
            "Updating proposal: {} (newleader), 0x{} (newzxid), {} (oldleader), 0x{} (oldzxid)",
            leader,
            Long.toHexString(zxid),
            proposedLeader,
            Long.toHexString(proposedZxid));

        proposedLeader = leader;
        proposedZxid = zxid;
        proposedEpoch = epoch;
    }
```
```java
    private void sendNotifications() {
        for (long sid : self.getCurrentAndNextConfigVoters()) {
            QuorumVerifier qv = self.getQuorumVerifier();
            ToSend notmsg = new ToSend(
                ToSend.mType.notification,
                proposedLeader,
                proposedZxid,
                logicalclock.get(),
                QuorumPeer.ServerState.LOOKING,
                sid,
                proposedEpoch,
                qv.toString().getBytes(UTF_8));

            LOG.debug(
                "Sending Notification: {} (n.leader), 0x{} (n.zxid), 0x{} (n.round), {} (recipient),"
                    + " {} (myid), 0x{} (n.peerEpoch) ",
                proposedLeader,
                Long.toHexString(proposedZxid),
                Long.toHexString(logicalclock.get()),
                sid,
                self.getMyId(),
                Long.toHexString(proposedEpoch));

            sendqueue.offer(notmsg);
        }
    }
```
<a name="PKSOv"></a>
###### 4. 通知交换直到找到 Leader 
```java
    while ((self.getPeerState() == ServerState.LOOKING) && (!stop)) {
        /*
         * Remove next notification from queue, times out after 2 times
         * the termination time
         */
        Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS);
        ...
    }
```

- **分支一：通知超时时间内，未收到选票（消息）**
```java
    if (n == null) {
        if (manager.haveDelivered()) {
            sendNotifications();
        } else {
            manager.connectAll();
        }

        /*
         * Exponential backoff
         */
        notTimeout = Math.min(notTimeout << 1, maxNotificationInterval);

        /*
         * When a leader failure happens on a master, the backup will be supposed to receive the honour from
         * Oracle and become a leader, but the honour is likely to be delay. We do a re-check once timeout happens
         *
         * The leader election algorithm does not provide the ability of electing a leader from a single instance
         * which is in a configuration of 2 instances.
         * */
        if (self.getQuorumVerifier() instanceof QuorumOracleMaj
                && self.getQuorumVerifier().revalidateVoteset(voteSet, notTimeout != minNotificationInterval)) {
            setPeerState(proposedLeader, voteSet);
            Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch);
            leaveInstance(endVote);
            return endVote;
        }

        LOG.info("Notification time out: {} ms", notTimeout);

    }
```
> 注释翻译：
> 当首领失败发生在 master 人身上时，后援本应获得神谕的荣誉并成为首领，但荣誉可能会推迟。一旦超时，我们会重新检查领导者选举算法是否无法从2个实例的配置中的单个实例中选举领导者。

请注意，Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch);<br />endVote 中的状态值（proposedLeader 等）可能被其他线程异步更新。

- **分支二：接收到来自其他节点的选票（消息），并且校验合法**
```java
    else if (validVoter(n.sid) && validVoter(n.leader)) {
        /*
         * Only proceed if the vote comes from a replica in the current or next
         * voting view for a replica in the current or next voting view.
         */
        switch (n.state) {    
        ...
        }
}
```
**通知状态一：LOOKING**<br />前提条件：

- 外部投票和本地服务器状态 ： LOOKING
- 外部选票所属服务器，以及它对应的 Leader 服务器必须在 Quorum 服务集群中
```java
    // If notification > current, replace and send messages out
    if (n.electionEpoch > logicalclock.get()) {
        logicalclock.set(n.electionEpoch);
        recvset.clear();
        if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) {
            updateProposal(n.leader, n.zxid, n.peerEpoch);
        } else {
            updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());
        }
        sendNotifications();
    } else if (n.electionEpoch < logicalclock.get()) {
            LOG.debug(
                "Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x{}, logicalclock=0x{}",
                Long.toHexString(n.electionEpoch),
                Long.toHexString(logicalclock.get()));
        break;
    } else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) {
        updateProposal(n.leader, n.zxid, n.peerEpoch);
        sendNotifications();
    }
```

- 如果外部选票选举纪元大于内部选票（使用逻辑时钟 logicalclock 表示）
   - 当前逻辑时钟 logicalclock 设置为外部选票选举纪元 - logicalclock.set(n.electionEpoch);
   - 清除已接受外部选票（vote.electionEpoch == self.electionEpoch）
   - 比较外部选票 leader、 zxid 以及 peerEpoch 与当前服务器 ID，zxid 以及 peerEpoch
      - 如果前者 order 更高的话，将外部选票中的 proposedLeader, proposedZxid 和 proposedEpoch 状态更新到当前选票
      - 如果后者 order 更高的话，将本地状态更新到当前 proposedLeader, proposedZxid 和 proposedEpoch
   - 发送当前选票到其他 Peers 上
- 如果外部选票选举纪元小于内部选票，直接忽略外部选票，接收下一个选票
   - 记录日志
- 如果外部选票选举纪元等于内部选票，进入选票对比流程：
   - 选票对比 - totalOrderPredicate
      - 如果外部选票优先于本地选票，将外部选票中的 proposedLeader, proposedZxid 和 proposedEpoch 状态更新到当前选票
      - 如果外部选票低于或等于本地选票，不予处理
   - 发送当前选票到其他 Peers 上

<a name="AaSeM"></a>
##### 选票全序比较 - totalOrderPredicate 
```java
    protected boolean totalOrderPredicate(long newId, long newZxid, long newEpoch, long curId, long curZxid, long curEpoch) {
        LOG.debug(
            "id: {}, proposed id: {}, zxid: 0x{}, proposed zxid: 0x{}",
            newId,
            curId,
            Long.toHexString(newZxid),
            Long.toHexString(curZxid));

        if (self.getQuorumVerifier().getWeight(newId) == 0) {
            return false;
        }

        /*
         * We return true if one of the following three cases hold:
         * 1- New epoch is higher
         * 2- New epoch is the same as current epoch, but new zxid is higher
         * 3- New epoch is the same as current epoch, new zxid is the same
         *  as current zxid, but server id is higher.
         */

        return ((newEpoch > curEpoch)
                || ((newEpoch == curEpoch)
                    && ((newZxid > curZxid)
                        || ((newZxid == curZxid)
                            && (newId > curId)))));
    }

```

- Epoch 更好者优先
   - 否则 Zxid 更高者优先
      - 否则 SID 更高者优先
<a name="mA5vb"></a>
### 仲裁连接管理器 - QuorumCnxManager
此类使用TCP实现用于领导人选举的连接管理器。它为每对服务器维护一个连接。棘手的部分是确保每对正确运行的服务器都有一个连接，并且可以通过网络进行通信。如果两台服务器试图同时启动一个连接，那么连接管理器会使用一种非常简单的平局打破机制，根据双方的IP地址来决定要断开哪个连接。对于每个对等点，管理器都维护一个要发送的消息队列。如果与任何特定对等方的连接断开，则发送方线程会将消息重新放回列表中。由于此实现目前使用队列实现来维护要发送到另一个对等点的消息，因此我们将消息添加到队列的尾部，从而更改消息的顺序。尽管这对领导人选举来说不是问题，但在巩固同行沟通时可能会出现问题。不过，这一点有待验证。


<a name="h4qsV"></a>
### 请求处理器 - org.apache.zookeeper.server.RequestProcessor
RequestProcessors被链接在一起以处理事务。请求总是按顺序处理。独立服务器、追随者和领导者都有稍微不同的RequestProcessors链接在一起。请求总是通过RequestProcessors链向前移动。请求通过processRequest（）传递给RequestProcessor。通常情况下，方法总是由单个线程调用。当调用shutdown时，请求RequestProcessor还应关闭与其连接的任何RequestProcessors。
<a name="KpF2H"></a>
#### 预处理器 - org.apache.zookeeper.server.PrepRequestProcessor
消息处理程序的多线程实现。Messenger实现了两个子类：WorkReceiver和WorkSender。从名称中可以明显看出每一个的功能。每一个都会产生一个新的线程。
<a name="xVruu"></a>
#### 事务投票请求器 - org.apache.zookeeper.server.quorum.ProposalRequestProcessor
此RequestProcessor只是将请求转发到AckRequestProcessor和SyncRequestProcessor。
<a name="Y2FC0"></a>
##### 请求处理 - processRequest
```java
    public void processRequest(Request request) throws RequestProcessorException {
        /* In the following IF-THEN-ELSE block, we process syncs on the leader.
         * If the sync is coming from a follower, then the follower
         * handler adds it to syncHandler. Otherwise, if it is a client of
         * the leader that issued the sync command, then syncHandler won't
         * contain the handler. In this case, we add it to syncHandler, and
         * call processRequest on the next processor.
         */
        if (request instanceof LearnerSyncRequest) {
            zks.getLeader().processSync((LearnerSyncRequest) request);
        } else {
            if (shouldForwardToNextProcessor(request)) {
                nextProcessor.processRequest(request); // 执行下一个 RequestProcessor
                                                       //  CommitProcessor         
            }
            if (request.getHdr() != null) {
                // We need to sync and get consensus on any transactions
                try {
                    zks.getLeader().propose(request);
                } catch (XidRolloverException e) {
                    throw new RequestProcessorException(e.getMessage(), e);
                }
                syncProcessor.processRequest(request); // 执行 SyncRequestProcessor 以及
                                                       // 下一个 AckReuqestProcessor
                                                       
            }
        }
    }
```
<a name="Qt9Qj"></a>
#### 同步请求处理器 - org.apache.zookeeper.server.SyncRequestProcessor
此RequestProcessor将请求记录到磁盘。它对请求进行批处理，以有效地执行io。在日志同步到磁盘之前，请求不会传递给下一个RequestProcessor。SyncRequestProcessor用于3种不同的情况：

1. Leader-将请求同步到磁盘，并将其转发给AckRequestProcessor，后者将ack发送回自身
2. Follower-将请求同步到磁盘，并将请求转发到SendAckRequestProcessor，后者将数据包发送到leader。SendAckRequestProcessor是可刷新的，允许我们强制将数据包推送到leader
3. Observer-将提交的请求同步到磁盘（作为INFORM数据包接收）。它从不将ack发送回leader，因此nextProcessor将为null。这改变了观察者上txnlog的语义，因为它只包含已提交的txns。

<a name="mMHFb"></a>
#### 提交请求处理器 - org.apache.zookeeper.server.quorum.CommitProcessor
此RequestProcessor将传入的已提交请求与本地提交的请求进行匹配。诀窍是，本地提交的更改系统状态的请求将作为传入的提交请求返回，因此我们需要将它们进行匹配。我们不只是等待提交的请求，而是处理属于其他会话的未提交请求。CommitProcessor是多线程的。线程之间的通信通过队列、原子和处理器上同步的wait/notifyAll来处理。CommitProcessor充当网关，允许请求继续处理管道的其余部分。它将允许许多读取请求，但只有一个写入请求同时在运行，从而确保写入请求按事务id顺序处理。-1个提交处理器主线程，它监视请求队列，并根据工作线程的sessionId将请求分配给工作线程，以便特定会话的读写请求始终分配给同一线程（因此保证按顺序运行）。-0-N个工作线程，它们在请求上运行请求处理器管道的其余部分。如果配置有0个工作线程，则主提交处理器线程将直接运行管道。典型的（默认）线程数是：在32核机器上，1个提交处理器线程和32个工作线程。多线程约束：-必须按顺序处理每个会话的请求。-写入请求必须按zxid顺序处理-必须确保一个会话中的写入之间没有竞争条件，这将触发另一个会话的读取请求设置的监视。当前的实现通过简单地不允许读取请求与写入请求并行处理来解决第三个限制。

<a name="wPpfU"></a>
#### Leader 确认请求处理器 - org.apache.zookeeper.server.quorum.AckRequestProcessor
这是一个非常简单的RequestProcessor，它只需将前一阶段的请求作为ACK转发给领导者。<br />Leader 服务器本地状态的处理。
<a name="ZPjid"></a>
##### Leader 成员 - org.apache.zookeeper.server.quorum.Leader
<a name="tg9po"></a>
###### 处理确认 - processAck
```java
   public synchronized void processAck(long sid, long zxid, SocketAddress followerAddr) {
        if (!allowedToCommit) {
            return; // last op committed was a leader change - from now on
        }
        // the new leader should commit
        if (LOG.isTraceEnabled()) {
            LOG.trace("Ack zxid: 0x{}", Long.toHexString(zxid));
            for (Proposal p : outstandingProposals.values()) {
                long packetZxid = p.packet.getZxid();
                LOG.trace("outstanding proposal: 0x{}", Long.toHexString(packetZxid));
            }
            LOG.trace("outstanding proposals all");
        }

        if ((zxid & 0xffffffffL) == 0) {
            /*
             * We no longer process NEWLEADER ack with this method. However,
             * the learner sends an ack back to the leader after it gets
             * UPTODATE, so we just ignore the message.
             */
            return;
        }

        if (outstandingProposals.size() == 0) {
            LOG.debug("outstanding is 0");
            return;
        }
        if (lastCommitted >= zxid) {
            LOG.debug(
                "proposal has already been committed, pzxid: 0x{} zxid: 0x{}",
                Long.toHexString(lastCommitted),
                Long.toHexString(zxid));
            // The proposal has already been committed
            return;
        }
        Proposal p = outstandingProposals.get(zxid);
        if (p == null) {
            LOG.warn("Trying to commit future proposal: zxid 0x{} from {}", Long.toHexString(zxid), followerAddr);
            return;
        }

        if (ackLoggingFrequency > 0 && (zxid % ackLoggingFrequency == 0)) {
            p.request.logLatency(ServerMetrics.getMetrics().ACK_LATENCY, Long.toString(sid));
        }

        p.addAck(sid);

        boolean hasCommitted = tryToCommit(p, zxid, followerAddr);

        // If p is a reconfiguration, multiple other operations may be ready to be committed,
        // since operations wait for different sets of acks.
        // Currently we only permit one outstanding reconfiguration at a time
        // such that the reconfiguration and subsequent outstanding ops proposed while the reconfig is
        // pending all wait for a quorum of old and new config, so its not possible to get enough acks
        // for an operation without getting enough acks for preceding ops. But in the future if multiple
        // concurrent reconfigs are allowed, this can happen and then we need to check whether some pending
        // ops may already have enough acks and can be committed, which is what this code does.

        if (hasCommitted && p.request != null && p.request.getHdr().getType() == OpCode.reconfig) {
            long curZxid = zxid;
            while (allowedToCommit && hasCommitted && p != null) {
                curZxid++;
                p = outstandingProposals.get(curZxid);
                if (p != null) {
                    hasCommitted = tryToCommit(p, curZxid, null);
                }
            }
        }
    }
```
<a name="gUGBb"></a>
#### Follower 确认请求处理器 - org.apache.zookeeper.server.quorum.SendAckRequestProcessor
Follower 确认专用请求处理器
<a name="PQeev"></a>
#### To Be Applied 列表请求处理器 - org.apache.zookeeper.server.quorum.Leader.ToBeAppliedRequestProcessor
此请求处理器只需维护toBeApplied列表。要使其工作，下一步必须是FinalRequestProcessor和FinalRequestProcessor.prrocessRequest必须同步处理请求！

<a name="KLWBm"></a>
#### 最终请求处理器 - org.apache.zookeeper.server.FinalRequestProcessor
这个请求处理器实际上应用与请求相关联的任何事务，并为任何查询提供服务。它总是位于RequestProcessor链的末尾（因此得名），因此它没有nextProcessor成员。此RequestProcessor依靠ZooKeeperServer来填充ZooKeeper Server的未完成请求成员。


